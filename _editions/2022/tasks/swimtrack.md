---
# static info
layout: task
year: 2022
hide: true  <!-- # change this to false once you finish editing-->

# required info
title: Swimmers video tracking <!-- # add your title here-->
subtitle: <!-- # leave this blanck-->
blurb: <!-- # add the task blurb here-->
---

<!-- # please respect the structure below-->
*See the [MediaEval 2022 webpage](https://multimediaeval.github.io/editions/2022/) for information on how to register and participate.*

#### Task Description

Swimming is one of the most ancient, yet popular Olympic disciplines as it holds the second highest potential of gold medals (37) after athletics (48). It has a long tradition of being analyzed (\eg race time, lap time, rankings) due to official time recording devices. There is however little information at a more detailed level, \ie within laps or on the swimmers' speed and real-time motion, except for manually annotated datasets. 
Recent efforts in deep learning have proposed approaches that pave the way for automated fine-grained data extractions, but a very high level of accuracy and robustness is yet to be reached to rely on their application to any swimming pool or camera position.  

The goal of this challenge is to push the envelope of systems that accurately track swimmers' motion in a reliable way during elite competitions. Current state of the art in multi-object tracking is limited by the unusual nature of a swimmer's motion and large noise generated by the water. This challenge is divided into 5 independent tasks. Each of them contains its own set of input data, output format, and an evaluation metric. Participants are required to follow this format to get feedback and make improvements on their contribution. We will proceed with a classical evaluation protocol: expected metrics for each task will be tested with a first dataset validation which contains both input and ground truth. 
A second dataset test will be kept private by the organizers, to prevent participants from exploiting solutions to reach better results (\eg overfitting a model for each individual task). Thus, participants are free to use the validation in any way they want to build their solution, but they need to make sure it is not too specific to it. 

We will provide swimming videos, screenshots and sounds recorded during national and international competitions. Videos cover all the 4 swimming styles (Freestyle, Backstroke, Breaststroke, Butterfly, Medley), both genders (female, male) and principal race lengths (50m, 100m, 200m, 400m) for 50m-long swimming pools. They cover all the swimming phases (e.g. standing, diving, underwater, return and finish). The camera view parameters vary from static wide angle to zoomed using various camera types (GoPro 8, Blackmagic Pocket 6K and Panasonic HC-V750). Resolutions range from HD to 4K with variable frame rates across the recordings (between 25fps and up to 50fps). Despite those differences, the provided videos share the same MP4 format resulting from the same compression algorithm.

Task 1: Swimmer Position Detection

Task 2: Stroke Rate Detection

Task 3: Camera Registration

Task 4: Characters Recognition of Score Boards

Task 5: Sound detection

#### Motivation and background

#### Introduction

#### Target group

#### Data

#### Ground truth

#### Evaluation methodology

#### Quest for insight
Here are several research questions related to this challenge that participants can strive to answer in order to go beyond just looking at the evaluation metrics: 
* <!-- # First research question-->
* <!-- # Second research question-->
<!-- # and so on-->

#### References and recommended reading
<!-- # Please use the ACM format for references https://www.acm.org/publications/authors/reference-formatting (but no DOI needed)-->
<!-- # The paper title should be a hyperlink leading to the paper online-->

#### Task organizers
* <!-- # First organizer-->
* <!-- # Second organizer-->
<!-- # and so on-->

#### Task auxiliaries
<!-- # optional, delete if not used-->
* <!-- # First auxiliary-->
* <!-- # Second auxiliary-->
<!-- # and so on-->

#### Task Schedule
* XX XXX: Data release <!-- # Replace XX with your date. We suggest setting the date in June-July-->
* XX November: Runs due <!-- # Replace XX with your date. We suggest setting enough time in order to have enough time to assess and return the results by the Results returned deadline-->
* XX November: Results returned  <!-- Replace XX with your date. Latest possible should be 10 November-->
* 21 November: Working notes paper  <!-- Fixed. Please do not change.-->
* Beginning December: MediaEval 2022 Workshop <!-- Fixed. Please do not change. Exact date to be decided-->

#### Acknolwedgments
<!-- # optional, delete if not used-->
